<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
   
    <title>LowLight</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon_io/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
</head>

        <!-- you should probably put some google analytics / meta bits and pieces
	    here : PRH 
    <meta property="og:title" content="Paul Hill's Website"> etc.
      <script async src="https://www.googletagmanager.com/gtag/js?id=??????????"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', '??????????');
  </script>
    -->

<body id="page-top">
    <div class="page-container">
        <div class="inner">
            <div class="container">
                <div class="row">
                    <div class="row vertical-align">
  
                        <div class="col-md-10">
                            <div class="title">
                                <h1>Production Workflows for Low-light Environments </h1>
                                <h4>AI-based Image Processing and Computer Vision </h4>
                            </div>
                        </div>
			<div class="col-md-2">
			    <p><a href='https://www.bristol.ac.uk'><img src="../uob-logo.svg" width="100%" height="20%" alt=""></a></p>
			    <p><a href='https://www.bristol.ac.uk/vision-institute'><img src="../bvilogo.svg" width="100%" height="20%" alt=""></a></p>
			     <p><a href='https://vilab.blogs.bristol.ac.uk'><img src="../VIL_logo.svg" width="100%" height="20%" alt=""></a></p>
			    
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-md">
                <p><img src="imagegt.png" width="80%" /></p>
                <p>The human visual system operates using various opponent processes, present in both the retina and visual cortex. These processes heavily rely on distinctions in color, luminance, or motion to trigger salient reactions. Contrast, which refers to differences in luminance and/or color that enable the differentiation of objects, plays a crucial role in subjectively evaluating image quality. Images and videos captured in low-light conditions often exhibit poor quality and visibility due to limitations in shutter angles, high ISO resulting in noise, and spectral biasing toward blue. Traditional enhancement techniques tend to wash out details, flatten the appearance, and amplify noise.</p>

                <p>This project aims to develop and validate a perceptually inspired deep learning framework for joint restoration of noisy, low light content (targeting natural history filmmaking) ensuring temporal consistency in terms of colour, luminance and motion.</p>

        <div class="row">
        <div class="col">

        <dt>Funder</dt>
            <dd>UKRI MyWorld Strength in Places Programme (SIPF00006/1), BRISTOL+BATH CREATIVE R+D (AH/S002936/1).</dd>
        </div>
       
        </div>
        <hr />
        <div class="row">
        <div class="col">
        <h2 id="downloads">Research team</h2>
            <h5 id="coreteam">Core</h5>
            <ul>
                <li><a href='https://pui-nantheera.github.io/'>N. Anantrasirichai</a> and <a href='https://david-bull.github.io/'>D.R. Bull</a>: Lead academics</li>
                <li><a href='https://research-information.bris.ac.uk/en/persons/alexandra-malyugina'>Alexandra Malyugina</a>: Researcher on low-light image denoising [<a href='https://malalejandra.github.io/low-light-dataset/'>Project page</a>] [<a href="https://www.sciencedirect.com/science/article/pii/S016516842300155X">Paper</a>]</li>
                <li><a href='https://danier97.github.io/'>Duolikun Danier</a>: Researcher on low-light video enhancement</li>
                <li><a href='https://research-information.bris.ac.uk/en/persons/crispian-morris'>Crispian Morris</a>: PhD on low-ight autofocus for advanced wildlife coverage (with BBC R&D)</li> 
                <li><a href='https://research-information.bris.ac.uk/en/persons/rachel-lin'>Rachel Lin</a>: PhD on low-light video enhancement</li> 
                <li><a href='https://research-information.bris.ac.uk/en/persons/joanne-lin'>Joanne Lin</a>: PhD on segmentation and object tracking in low-light environment</li>
            </ul>
            <h5 id="UGteam">Undergrad/Postgrad projects</h5>
            <ul>
                <li>Anastasia Yi (2023), A comprehensive study of object tracking in low-light environments [<a href='https://www.dropbox.com/scl/fi/l3oh8sorwud05e3xxy7sb/Anastasia-Yi-Thesis.pdf?rlkey=68z07mtcl0oiyg4xzbzrl4knt&dl=0'>Thesis</a>]</li>
                <li>Siyu Zhou (2023), Temporal consistency in low-light video enhancement [<a href='https://www.dropbox.com/scl/fi/ie3d9fqeebf82pqv8l2jj/Evangeline-Zhou-Thesis.pdf?rlkey=l1hcs160dccfenecqysguahen&dl=0'>Thesis</a>]</li>
                <li>Felicia Dubicki-Piper (2023), VideoINR+: Video denoising with implicit neural representation [<a href='https://github.com/FelixxDP/VideoINR-Plus'>Code</a>]</li>
            </ul>
        </div>
       
        </div>
           
        
            </div>
            <hr />
            <div class="container-md">
                <h2 id="downloads">Downloads</h2>
                <h5 id="UGteam">Publications</h5>
                <ul>
                    <li><a>Topological Loss Function for Image Denoising on a new BVI-lowlight Dataset</a>. A. Malyugina, N. Anantrasirichai, and D. Bull. Signal Processing. 2023. [<a href="https://www.sciencedirect.com/science/article/pii/S016516842300155X">PDF</a>]</li>
                    <li><a>Contextual colorization and denoising for low-light ultra high resolution sequences</a>. N. Anantrasirichai and D. Bull.  In Proceedings of the IEEE International Conference on Image Processing. 2021 [<a href="https://arxiv.org/pdf/2101.01597.pdf">PDF</a>] [<a href="https://pui-nantheera.github.io/research/CIC/colorization_denoising.html">Project page</a>] </li>
                </ul>
                <h5 id="UGteam">Datasets</h5>
                <ul>
                    <li>Paired image dataset for image denoising <a href="https://ieee-dataport.org/open-access/bvi-lowlight-fully-registered-datasets-low-light-image-and-video-enhancement">[dataset]</a> <a href="https://www.sciencedirect.com/science/article/pii/S016516842300155X">[paper]</a></li>
                    <li>Paired video dataset for low-light enhancement <a href="https://ieee-dataport.org/open-access/bvi-lowlight-fully-registered-datasets-low-light-image-and-video-enhancement">[dataset]</a></li>
                    <li>Unpaired UHD video dataset  <a href="https://pui-nantheera.github.io/research/CIC/colorization_denoising.html">[dataset]</a> <a href="https://ieeexplore.ieee.org/document/9506694">[paper]</a> </li>
                </ul>
            </div>
            <hr />
            <div class="container-md">
                <h2 id="related">Related research</h2>
                <h5 id="downloads">Related publications from VI-Lab</h5>
                <ul>
                    <li><a href="https://arxiv.org/pdf/2007.12391.pdf">Artificial intelligence in the creative industries: A review</a>. N Anantrasirichai and D R Bull, Artif Intell Rev 55, 2022</li>
                    <li><a href="https://arxiv.org/abs/2302.08455">ST-MFNet Mini: Knowledge distillation-driven frame interpolation</a>. C Morris, D Danier, F Zhang, N Anantrasirichai, D R Bull. IEEE International Conference on Image Processing. 2023 </li>
                </ul>
                <h5 id="downloads">Denoising in different modalities</h5>
                <ul>
                    <li><a href="https://seis.bristol.ac.uk/~eexna/papers/retinal_img_enhance.pdf">Adaptive-weighted bilateral filtering for optical coherence tomography</a></li>
                    <li><a href="https://ieeexplore.ieee.org/document/9323696">Despeckling SAR Images of the Sea Surface</a></li>
                    <li><a href="https://ieeexplore.ieee.org/abstract/document/9160976">Solving SAR Imaging Inverse Problems Using Nonconvex Regularization</a></li>
                    <li><a href="https://www-sigproc.eng.cam.ac.uk/foswiki/pub/Main/NGK/Anantrasirichai_etal_draft_jrnl_12jul2012_c.pdf">Mitigating The Effects of Atmospheric Distortion</a></li>
                </ul>
            </div>
          

        </div>
    </div>
</body>

</html>
