<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
   
    <title>MyUnderwaterWorld</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="apple-touch-icon" sizes="180x180" href="../favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../favicon_io/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
</head>

        <!-- you should probably put some google analytics / meta bits and pieces
	    here : PRH 
    <meta property="og:title" content="Paul Hill's Website"> etc.
      <script async src="https://www.googletagmanager.com/gtag/js?id=??????????"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', '??????????');
  </script>
    -->

<body id="page-top">
    <div class="page-container">
        <div class="inner">
            <div class="container">
                <div class="row">
                    <div class="row vertical-align">
  
                        <div class="col-md-10">
                            <div class="title">
                                <h1>MyUnderwaterWorld: Intelligent Underwater Scene Representation</h1>
                                <h4>Underwater Scene Enhancement and 3D Modelling</h4>
                            </div>
                        </div>
			<div class="col-md-2">
			    <p><a href='https://www.bristol.ac.uk'><img src="../uob-logo.svg" width="100%" height="20%" alt=""></a></p>
			    <p><a href='https://www.bristol.ac.uk/vision-institute'><img src="../bvilogo.svg" width="100%" height="20%" alt=""></a></p>
			     <p><a href='https://vilab.blogs.bristol.ac.uk'><img src="../VIL_logo.svg" width="100%" height="20%" alt=""></a></p>
			    
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-md">
                <p><img src="underwater_enh.png" width="100%" /></p>
                <p align="right">(Left) Raw video by Piyapong Suwannakul and (Right) Processed video using <a href="https://pui-nantheera.github.io/research/CIC/colorization_denoising.html" target="_blank">VI-Lab tools</a>. See video <a href="https://www.youtube.com/embed/sLL2D--731M" target="_blank">HERE</a></p>

                <h2 id="aim">Aim</h2>
                <p>Our oceans have been explored for hundreds of years and these activities are becoming increasingly important because of the need to  manage and conserve mineral and biological resources effectively, as well as to better understand planetary-scale processes including tectonics and marine hazards.  Exploration and analysis are however always limited by the number of diving experts, technologies, and in particular, costs. Advanced imaging methods now support a new paradigm of remote discovery where onshore experts with specific knowledge, such as geologists, archaeologists and biologists, are able to remotely model and explore underwater scenes. </p>

                <p>Underwater environment represents the combination of several challenges. Water is a dynamic medium and suspended particles move. Light scatter causes blur and halo effects, whilst light absorption leads to colour distortion and reduced contrast. The model of underwater imagery should thus comprise temporally- and spatially-variant distortion, uneven intensity bias,  multiplicative noise, and additive noise. This project aims to exploit underwater image priors to perform 3D mapping process can be done directly from the raw underwater sequences. </p>

        <div class="row">
        <div class="col">
        <dt>Collaborators</dt>
            <dd>National Park Service Submerged Resources Center, Woods Hole Oceanographic Institute, Marine Imaging Technologies, National Oceanic and Atmospheric Administration, Gates Underwater Products, Esprit Film & Television, Beam UK</dd>
        </div>
        </div>
        <div class="row">
        <div class="col">
        <dt>Funder</dt>
            <dd>EPSRC <a href='https://gtr.ukri.org/projects?ref=EP%2FY002490%2F1' target="_blank">ECR International Collaboration Grant</a> (EP/Y002490/1), UKRI <a href='https://www.myworld-creates.com/' target="_blank">MyWorld</a> Strength in Places Programme (SIPF00006/1), EPSRC IAA</dd>
        </div>
        </div>
        <hr />
       
        <h2 id="downloads">Methods</h2>
        <div class="row">
        <div class="col">
        
            <!-- <h5 id="coreteam">Core</h5> -->
            <ul>
                <li><a>RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction</a> (BMVC2025) <pdf><p>[</pdf><a href="https://arxiv.org/pdf/2505.15737">PDF</a><pdf>]</pdf> <pdf>[</pdf><a href="https://github.com/theflash987/RUSplatting">Code</a><pdf>]</pdf> [<a href='https://zenodo.org/records/15482420' target="_blank">Dataset</a>]</p>
                    <p align="left"><img src="https://github.com/theflash987/RUSplatting/blob/main/image/overview_final_v2.png?raw=true" width="100%" /></p>
                    <p>Our enhanced Gaussian Splatting framework improves both visual quality and geometric accuracy in underwater rendering. We employ physics-guided decoupled RGB learning for accurate colour restoration, a frame interpolation strategy with adaptive weighting to address sparse views, and a new loss function that reduces noise while preserving edges, crucial for deep-sea content.</p>
                </li>
                <li><a>SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</a> <p> <pdf> [</pdf><a href="">PDF</a><pdf>] </pdf>
                    <p align="left"><img src="SWAGSplating.jpg" width="100%" /></p>
                    <p>We present a semantic-guided 3D Gaussian Splatting framework for deep-sea scene reconstruction, where each Gaussian embeds CLIP-derived features to enforce semantic and structural consistency. A dedicated semantic loss and stage-wise training strategy further enhance stability and reconstruction fidelity.</p>
                </li>
                
            </ul>
          
        </div>
        <div class="col">
            <ul>
                <li><a>UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction</a> (WACV2025) 
                    <p>[<a href='https://wanghaoran16.github.io/UW-GS-website/' target="_blank">Project webpage</a>] [</pdf><a href="https://openaccess.thecvf.com/content/WACV2025/html/Wang_UW-GS_Distractor-Aware_3D_Gaussian_Splatting_for_Enhanced_Underwater_Scene_Reconstruction_WACV_2025_paper.html">PDF</a><pdf>]</pdf> <pdf>[</pdf><a href="https://github.com/WangHaoran16/UW-GS/tree/master">Code</a><pdf>]</pdf> [<a href="https://zenodo.org/records/14835876">Dataset</a>]</p>
                    <p align="left"><img src="UW-GS.png" width="100%" /></p><p>Our Gaussian Splatting-based method introduces a color appearance model for distance-dependent color variation, employs a new physics-based density control strategy to enhance clarity for distant objects, and uses a binary motion mask to handle dynamic content. The method is optimised with a well-designed loss function supporting scattering media and strengthened by pseudo-depth maps.</p>
                    </li>
                    <li><a>Marine Snow Removal Using Internally Generated Pseudo Ground Truth</a> (EUSIPCO2025) <p> <pdf> [</pdf><a href="https://arxiv.org/pdf/2504.19289">PDF</a><pdf>] </pdf>
                    <p align="middle"><img src="marinesnow.jpg" width="70%" /></p>
                    <p>The framework introduces a novel method for generating paired datasets from raw underwater videos, producing snowy and snow-free pairs that enable supervised training for video enhancement.</p>
                </li>
                
            </ul>
          
        </div>
        </div>
        <hr />
        <div class="row">
        <div class="col">
        <h2 id="downloads">Research team</h2>
            <h5 id="coreteam">Core</h5>
            <ul>
                <li><a href='https://pui-nantheera.github.io/'>N. Anantrasirichai</a>: Lead academic</li>
                <li><a href='https://russellllaputa.github.io/'>Guoxi Huang (Edward)</a>: Postdoctoral researcher, <pdf>[</pdf><a href="https://arxiv.org/pdf/2501.14265">Paper1</a><pdf>]</pdf> <pdf>[</pdf><a href="https://www.arxiv.org/abs/2505.01869">Paper2</a><pdf>]</pdf> [<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13460/1346009/BVI-Mamba--video-enhancement-using-a-visual-state-space/10.1117/12.3053998.short">Paper3</a>]</li> </li>
                <li><a href='https://research-information.bris.ac.uk/en/persons/alexandra-malyugina'>Alexandra Malyugina</a>: Postdoctoral researcher, [<a href="https://arxiv.org/abs/2504.19289">Paper</a>]</li>
                <li><a href=''>Haoran Wang</a>: PhD student (Co-supervisor: <a href='https://fan-aaron-zhang.github.io/' target="_blank">Aaron Zhang</a>), <pdf>[</pdf><a href="https://openaccess.thecvf.com/content/WACV2025/html/Wang_UW-GS_Distractor-Aware_3D_Gaussian_Splatting_for_Enhanced_Underwater_Scene_Reconstruction_WACV_2025_paper.html">Paper</a><pdf>]</pdf> [<a href='https://wanghaoran16.github.io/UW-GS-website/' target="_blank">UW-GS Project Page</a>]</li>
                <li><a href=''>Yini Li</a>: PhD student (Co-supervisor: <a href='https://david-bull.github.io/' target="_blank">Prof David Bull</a>), <pdf>[</pdf><a href="https://openaccess.thecvf.com/content/WACV2025/hthttps://arxiv.org/abs/2503.11175">Paper</a><pdf>]</pdf> </li>
            </ul>
            <h5 id="UGteam">Undergrad/Postgrad projects</h5>
            <ul>
                <li><span style="color:#C71585;">[Best AI Project Prize]</span> Zhuodong Jiang (2024/2025), Underwater 3D Gaussian Splatting With Frame Interpolation, Colour
Channel Decoupling and Adaptive Bilateral Filtering [<a href='https://www.dropbox.com/scl/fi/xpyp9sscf1bvzce6y2p02/Zhuodong-Jiang-Zhuodong_Jiang_Underwater_3D_Gaussian_Splatting_with_Frame_Interpolation.pdf?rlkey=qsbragyu9sjow6ya27bdxujf8&st=ws7n3cur&dl=0' target="_blank">Thesis</a>] [<a href='https://arxiv.org/pdf/2505.15737' target="_blank">Paper</a>] [<a href='https://zenodo.org/records/15482420' target="_blank">Submerged3D Dataset</a>]</li>
                <li>Luca Gough (2023/2024), 3D Representation of Underwater Scenes using Neural Radiance Fields [<a href='https://www.dropbox.com/scl/fi/9nqtjamc0v8qbwjrvsd8r/Luca-Gough-Thesis_Final.pdf?rlkey=imqqhqrvxueostxqceukxskhf&st=b6cpvqej&dl=0' target="_blank">Thesis</a>] [</pdf><a href="https://arxiv.org/abs/2502.16351">Paper</a><pdf>]</li>
                <li>George Atkinson (2023/2024), Generative Deep Learning for Temporally Consistent Underwater
Video Enhancement [<a href='https://www.dropbox.com/scl/fi/hc0cbdqadiqh1lrxo3nxj/George-Atkinson-dissertation_2_.pdf?rlkey=yuvb6xhklrko2v5vohffcfssp&st=b5xqm63l&dl=0' target="_blank">Thesis</a>]</li> 
            </ul>
        </div> 
        </div>
            </div>
            <hr />
            
            <div class="container-md">
                <h2 id="downloads">Downloads</h2>
                <h5 id="UGteam">Publications</h5> 
                <ul>
                    <li><a>RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction</a>. Z Jiang, H Wang, G Huang, B Seymour and N Anantrasirichai, 36th British Machine Vision Conference. 2025 <pdf>[</pdf><a href="https://arxiv.org/pdf/2505.15737">PDF</a><pdf>]</pdf> <pdf>[</pdf><a href="https://github.com/theflash987/RUSplatting">CODE</a><pdf>]</pdf> [<a href='https://zenodo.org/records/15482420' target="_blank">Dataset</a>]</li>
                    <li><a>UW-GS: Distractor-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction</a>. H Wang, N Anantrasirichai, F Zhang, and D Bull, IEEE/CVF Winter Conference on Applications of Computer Vision. 2025 <pdf>[</pdf><a href="https://openaccess.thecvf.com/content/WACV2025/html/Wang_UW-GS_Distractor-Aware_3D_Gaussian_Splatting_for_Enhanced_Underwater_Scene_Reconstruction_WACV_2025_paper.html">PDF</a><pdf>]</pdf> <pdf>[</pdf><a href="https://github.com/WangHaoran16/UW-GS/tree/master">CODE</a><pdf>]</pdf> [<a href='https://wanghaoran16.github.io/UW-GS-website/' target="_blank">UW-GS Project Page</a>] [<a href="https://zenodo.org/records/14835876">Dataset</a>]</li>
                    <li><a>AquaNeRF: Neural Radiance Fields in Underwater Media with Distractor Removal</a>. L Gough, A Azzarelli, F Zhang, and N Anantrasirichai, IEEE International Symposium on Circuits and Systems. 2025 <pdf>[</pdf><a href="https://arxiv.org/abs/2502.16351">PDF</a><pdf>]</pdf> </li>
                    <li><a>Marine Snow Removal Using Internally Generated Pseudo Ground Truth</a>. A Malyugina, G Huang, E Ruiz, B Leslie, N Anantrasirichai. 33rd European Signal Processing Conference, 2025 <pdf>[</pdf><a href="https://arxiv.org/abs/2504.19289">PDF</a><pdf>]</pdf> </li>
                    <li><span style="color:#C71585;">[Shortlist Best Student Paper Award]</span> <a>Zero-TIG: Temporal Consistency-Aware Zero-Shot Illumination-Guided Low-light Video Enhancement</a>. Y Li and N Anantrasirichai. 33rd European Signal Processing Conference, 2025 <pdf>[</pdf><a href="https://arxiv.org/pdf/2503.11175">PDF</a><pdf>]</pdf> </li>
                    <li><a>BVI-Mamba: video enhancement using a visual state-space model for low-light and underwater environments</a>. G Huang, R Lin, Y Li, D Bull, N Anantrasirichai. Machine Learning from Challenging Data, 2025 <pdf>[</pdf><a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13460/1346009/BVI-Mamba--video-enhancement-using-a-visual-state-space/10.1117/12.3053998.short">PDF</a><pdf>]</pdf> [<a href="https://github.com/russellllaputa/BVI-Mamba">CODE</a>]</li>
                </ul> 
                <h5 id="UGteam">White papers</h5>
                <ul>
                    <li><a>Visual enhancement and 3D representation for underwater scenes: a review</a>, G Huang, H Wang, B Seymour, E Kovacs, J Ellerbrock, D Blackham, N Anantrasirichai, 2025 [<a href="https://arxiv.org/pdf/2505.01869">PDF</a>] </li>
                    <li><a>Bayesian neural networks for one-to-many mapping in image enhancement</a>, G Huang, N Anantrasirichai, F Ye, Z Qi, R Lin, Q Yang, D Bull, 2025 [<a href="https://arxiv.org/pdf/2501.14265?">PDF</a>] [<a href="https://github.com/BinCVER/Bayesian-Enhancement-Model">CODE</a>]</li>
                    
                </ul>
                <h5 id="UGteam">Datasets</h5>
                <ul>
                    <li><a href="https://zenodo.org/records/11093417">BVI-Coral</a>: Underwater scenes for 3D reconstruction [<a href="https://zenodo.org/records/11093417">dataset</a>] </li>
                    <li><a href="https://zenodo.org/records/14835876">S-UW</a>: Underwater images from shallow water areas [<a href="https://zenodo.org/records/14835876">dataset</a>]</li>
                    <li><a href="https://zenodo.org/records/15482420">Submerged3D</a>: Deep underwater environments (in collaboration with National Park Service Submerged Resources Center) [<a href="https://zenodo.org/records/15482420">dataset</a>] </li>
                    
                </ul>
            </div>
            <hr />
            
            
            <div class="container-md">
                <h2 id="related">Related research</h2>
                <h5 id="downloads">Related publications from VI-Lab</h5>
                <ul>
                    <li><a href="https://pui-nantheera.github.io/research/CIC/colorization_denoising.html" target="_blank">A unified framework for contextual lighting, colorization and denoising for UHD sequences </a>. N Anantrasirichai and D R Bull. IEEE ICIP, 2021</li>
                    <li><a href="https://arxiv.org/pdf/2007.12391.pdf">Artificial intelligence in the creative industries: A review</a>. N Anantrasirichai and D R Bull, Artif Intell Rev 55, 2022</li>
                    <li><a href="https://arxiv.org/abs/2302.08455">ST-MFNet Mini: Knowledge distillation-driven frame interpolation</a>. C Morris, D Danier, F Zhang, N Anantrasirichai, D R Bull. IEEE International Conference on Image Processing. 2023 </li>
                </ul>
                <h5 id="downloads">Denoising in different modalities</h5>
                <ul>
                    <li><a href="https://seis.bristol.ac.uk/~eexna/papers/retinal_img_enhance.pdf">Adaptive-weighted bilateral filtering for optical coherence tomography</a></li>
                    <li><a href="https://ieeexplore.ieee.org/document/9323696">Despeckling SAR Images of the Sea Surface</a></li>
                    <li><a href="https://ieeexplore.ieee.org/abstract/document/9160976">Solving SAR Imaging Inverse Problems Using Nonconvex Regularization</a></li>
                    <li><a href="https://www-sigproc.eng.cam.ac.uk/foswiki/pub/Main/NGK/Anantrasirichai_etal_draft_jrnl_12jul2012_c.pdf">Mitigating The Effects of Atmospheric Distortion</a></li>
                </ul>
            </div>
          

        </div>
    </div>
</body>

</html>
